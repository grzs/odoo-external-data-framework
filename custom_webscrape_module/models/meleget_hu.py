# coding: utf-8
import re
import requests
from time import strptime, mktime
from datetime import datetime
from bs4 import BeautifulSoup as bs

from odoo import api, fields, models
from odoo.exceptions import ValidationError

import logging
_logger = logging.getLogger(__name__)

# Ignoring pyOpenSSL warnings
from cryptography.utils import CryptographyDeprecationWarning
import warnings
warnings.simplefilter('ignore', category=CryptographyDeprecationWarning)


class ExternalDataSourceMelegetHu(models.Model):
    _inherit = 'external.data.source'

    @api.model
    def _selection_data_source_type(self):
        selection = super(
            ExternalDataSourceMelegetHu, self)._selection_data_source_type()
        selection.append(('webscrape.meleget.hu', 'meleget.hu'))
        return selection


class WebscrapeMelegetHu(models.Model):
    _name = 'webscrape.meleget.hu'
    _description = "meleget.hu"
    _inherits = {'external.data.source': 'data_source_id'}

    data_source_id = fields.Many2one(
        'external.data.source',
        string="Data source",
        required=True,
        ondelete='cascade',
    )
    data_source_type_is_set = fields.Boolean(
        "Scraper linked",
        readonly=True,
        store=True,
    )
    base_url = fields.Char("Base URL", required=True)

    def create(self, *args, **kwargs):
        res = super(WebscrapeMelegetHu, self).create(*args, **kwargs)
        res._set_data_source_type()
        return res

    @api.depends('data_source_id')
    def _set_data_source_type(self):
        for record in self:
            if not record.data_source_id.data_source_type_id:
                record.data_source_type_is_set = False
                record.data_source_id.data_source_type_id = record
            record.data_source_type_is_set = True

    def fetch_package_data(self):
        """Parsing <base_url>/sitemap.xml (generated by Google)
        with BeautifulSoup xml parser. Returns a list of dictionaries."""

        # sanitize base_url and compose sitemap_url
        match_url = re.match('https?://[^/]+', self.base_url)
        if match_url:
            base_url = match_url.group()
        else:
            raise ValidationError(f"Invalid base URL: {self.base_url}")
        sitemap_url = '/'.join([base_url, 'sitemap.xml'])

        # download sitemap and create bs object
        sitemap = bs(requests.get(sitemap_url).text, 'xml')

        _logger.info(f"Processing sitemap for {self.base_url}")
        time_pattern = '%Y-%m-%dT%H:%M:%S%z'
        packages = []
        for pkg in sitemap.contents[0].children:
            last_mod = (datetime.fromtimestamp(
                mktime(strptime(pkg.lastmod.text, time_pattern)))
                if pkg.lastmod else None
                )
            if self.last_fetch and last_mod and self.last_fetch > last_mod:
                continue
            # path = pkg.loc.text[len(base_url):].split('/')
            # level = len(path)
            # changefreq = pkg.changefreq.text if pkg.changefreq else None
            priority = float(pkg.priority.text) if pkg.priority else None
            packages.append({
                "name": pkg.loc.text,
                "priority": priority,
                "last_mod": last_mod,
            })
        return packages

    def pull_package(self, url):
        """Parsing a webpage passed in a 'package' object. Returns parsed data.
        """

        # scrape page with BeautifulSoup
        soup = bs(requests.get(url).text, "lxml")
        base_url = re.match('https?://[^/]+', url).group()
        data = {}

        # opengraph
        # meta_og_type = soup.head.find("meta", property="og:type")

        # breadcrumbs (=categories)
        data_category = []
        bc_parent = None
        breadcrumbs_bs = soup.body.find(
            itemtype='http://schema.org/BreadcrumbList'
        )
        for bc in self._breadcrumbs(breadcrumbs_bs):
            href = bc.get("item")
            if not href or href == url or href == base_url:
                continue

            data_category.append({
                "href": href,
                "name": bc.get("name"),
                "parent_href": bc_parent,
            })
            bc_parent = href
        data.update(category=data_category)

        # append specific data objects to list
        if soup.find(itemtype='//schema.org/Product'):
            data_product = self._parse_product_data(soup)
            if data_category:
                data_product.update({
                    'category_href': data_category[-1].get('href'),
                })
            data.update(product=[data_product])

        # TODO: attachments
        # attachment_list = list(
        #     self._attachment_list(soup.select_one("div.attached_documents"))
        # )

        # TODO: parse parameter table as product attributes
        # parameter_table = list(self._parse_parameter_table(soup))

        return data

    @api.model
    def _parse_product_data(self, soup):
        res = {}

        schemaorg_product = soup.find(itemtype='//schema.org/Product')
        product_props = [
            ("name", "string"),
            ("description", "tag"),
            ("brand", "string"),
            ("sku", "content"),
            ("image", "src"),
        ]
        for p in product_props:
            if p[1] == "string":
                val = schemaorg_product.findChild(itemprop=p[0]).string.strip()
            elif p[1] == "tag":
                val = str(schemaorg_product.findChild(itemprop=p[0]))
            else:
                val = schemaorg_product.findChild(itemprop=p[0])[p[1]]
            res[p[0]] = val if val else None

        # price, availability
        schemaorg_offer = soup.find(itemtype='//schema.org/Offer')
        price_original = soup.select_one("span.product_table_original")
        try:
            offer_props = [
                ("pricecurrency", "content"),
                ("price", "content"),
                ("availability", "href"),
                ("category", "content"),
            ]
            for p in offer_props:
                val = schemaorg_offer.findChild(itemprop=p[0]).get(p[1])
                res[p[0]] = val if val else None

            if price_original:
                found = re.findall('[0-9]+', price_original.string)
                res.update(price_original=''.join(found) if found else None)
        except AttributeError as e:
            _logger.error("schema.org/Offer missing error:" + str(e))
            addtocart_a = soup.find("a", id="add_to_cart")
            if addtocart_a:
                res.update({
                    "price": addtocart_a.get("data-price-without-currency"),
                    "pricecurrency": addtocart_a.get("data-currency"),
                })

        return res

    @api.model
    def _parameter_table(self, soup):
        param_table = soup.body.select_one("table.parameter_table")
        if not param_table:
            return False

        for row in param_table.findChildren("tr"):
            cells = row.findChildren("td")
            if not cells[0].string:
                continue
            yield {
                cells[0].string.strip():
                cells[1].string.strip() if cells[1].string else None,
            }

    @api.model
    def _attachment_list(self, bs_list):
        if not bs_list:
            return False

        for a in bs_list.find_all("a"):
            yield {
                "name": a.string.strip(),
                "url": a["href"],
            }

    @api.model
    def _breadcrumbs(self, bs_breadcrumbs):
        if (
                not bs_breadcrumbs or
                bs_breadcrumbs['itemtype'] !=
                'http://schema.org/BreadcrumbList'
        ):
            return False

        for elem in bs_breadcrumbs.find_all(itemprop="itemListElement"):
            yield {
                "item": elem.find(itemprop="item")["href"],
                "name": elem.find(itemprop="name").string.strip(),
                "position": int(elem.find(itemprop="position")["content"]),
            }
