# coding: utf-8
import logging
import re
import requests as req
from time import strptime, mktime
from datetime import datetime
from bs4 import BeautifulSoup as bs

from odoo import api, fields, models
from odoo.exceptions import ValidationError

_logger = logging.getLogger(__name__)


class WebscrapeScraperMelegetHu(models.Model):
    _name = 'webscrape.scraper.meleget.hu'
    _description = "meleget.hu"
    _inherits = {'webscrape.site': 'site_id'}

    scraper_is_set = fields.Boolean(
        "Scraper linked",
        readonly=True,
    )

    def _test(self):
        return "Hello from scraper!"

    def write(self, *args, **kwargs):
        res = super(WebscrapeScraperMelegetHu, self).write(*args, **kwargs)
        self._set_scraper()
        return res

    @api.depends('site_id')
    def _set_scraper(self):
        for record in self:
            if not record.scraper_is_set:
                record.site_id.scraper_model = record._name
                record.site_id.scraper_id = record.id
                record.scraper_is_set = True

    def process_sitemap(self):
        """Parsing <base_url>/sitemap.xml (generated by Google)
        with BeautifulSoup xml parser. Returns a list of dictionaries."""

        # sanitize base_url and compose sitemap_url
        match_url = re.match('https?://[^/]+', self.base_url)
        if match_url:
            base_url = match_url.group()
        else:
            raise ValidationError(f"Invalid base URL: {self.base_url}")
        sitemap_url = '/'.join([base_url, 'sitemap.xml'])

        # download sitemap and create bs object
        sitemap = bs(req.get(sitemap_url).text, 'xml')

        _logger.info("Processing sitemap...")
        time_pattern = '%Y-%m-%dT%H:%M:%S%z'
        pages = []
        for page in sitemap.contents[0].children:
            last_mod = (datetime.fromtimestamp(
                mktime(strptime(page.lastmod.text, time_pattern)))
                if page.lastmod else None
                )
            if self.last_fetch and last_mod and self.last_fetch > last_mod:
                continue
            path = page.loc.text[len(base_url):].split('/')
            pages.append({
                "url": page.loc.text,
                "level": len(path),
                "changefreq":
                page.changefreq.text if page.changefreq else None,
                "priority":
                float(page.priority.text) if page.priority else None,
                "last_mod": last_mod,
            })
        return pages

    def scrape_page(self, url):
        """Parsing a webpage passed in a 'page' object. Returns parsed data.
        """

        # scrape page with BeautifulSoup
        soup = bs(req.get(url).text, "lxml")
        base_url = re.match('https?://[^/]+', url).group()
        data = []

        # opengraph
        # meta_og_type = soup.head.find("meta", property="og:type")

        # breadcrumbs (=categories)
        bc_parent = None
        for bc in self._breadcrumbs(
                soup.body.find(itemtype='http://schema.org/BreadcrumbList')
        ):
            href = bc.get("item")
            if not href or href == url or href == base_url:
                continue

            data.append({
                "content_type": "category",
                "vals": {
                    "href": href,
                    "name": bc.get("name"),
                    "parent_href": bc_parent,
                },
            })
            bc_parent = href
        last_category_href = data[-1]["vals"]["href"] if data else False

        # append specific data objects to list
        if soup.find(itemtype='//schema.org/Product'):
            product_vals = self._parse_product_data(soup)
            if last_category_href:
                product_vals.update(category=last_category_href)

            data.append({
                "content_type": "product",
                "vals": product_vals,
            })

        # TODO: attachments
        # attachment_list = list(
        #     self._attachment_list(soup.select_one("div.attached_documents"))
        # )

        # TODO: parse parameter table as product attributes
        # parameter_table = list(self._parse_parameter_table(soup))

        return data

    @api.model
    def _parse_product_data(self, soup):
        res = {}

        schemaorg_product = soup.find(itemtype='//schema.org/Product')
        product_props = [
            ("name", "string"),
            ("description", "tag"),
            ("brand", "string"),
            ("sku", "content"),
            ("image", "src"),
        ]
        for p in product_props:
            if p[1] == "string":
                val = schemaorg_product.findChild(itemprop=p[0]).string.strip()
            elif p[1] == "tag":
                val = str(schemaorg_product.findChild(itemprop=p[0]))
            else:
                val = schemaorg_product.findChild(itemprop=p[0])[p[1]]
            res[p[0]] = val if val else None

        # price, availability
        schemaorg_offer = soup.find(itemtype='//schema.org/Offer')
        price_original = soup.select_one("span.product_table_original")
        try:
            offer_props = [
                ("pricecurrency", "content"),
                ("price", "content"),
                ("availability", "href"),
                ("category", "content"),
            ]
            for p in offer_props:
                val = schemaorg_offer.findChild(itemprop=p[0]).get(p[1])
                res[p[0]] = val if val else None

            if price_original:
                found = re.findall('[0-9]+', price_original.string)
                res.update(price_original=''.join(found) if found else None)
        except AttributeError as e:
            print(e)
            # TODO: log error "'NoneType' object has no attribute 'findChild'"
            addtocart_a = soup.find("a", id="add_to_cart")
            if addtocart_a:
                res.update({
                    "price": addtocart_a.get("data-price-without-currency"),
                    "pricecurrency": addtocart_a.get("data-currency"),
                })

        return res

    @api.model
    def _parameter_table(self, soup):
        param_table = soup.body.select_one("table.parameter_table")
        if not param_table:
            return False

        for row in param_table.findChildren("tr"):
            cells = row.findChildren("td")
            if not cells[0].string:
                continue
            yield {
                cells[0].string.strip():
                cells[1].string.strip() if cells[1].string else None,
            }

    @api.model
    def _attachment_list(self, bs_list):
        if not bs_list:
            return False

        for a in bs_list.find_all("a"):
            yield {
                "name": a.string.strip(),
                "url": a["href"],
            }

    @api.model
    def _breadcrumbs(self, bs_breadcrumbs):
        if (
                not bs_breadcrumbs or
                bs_breadcrumbs['itemtype'] !=
                'http://schema.org/BreadcrumbList'
        ):
            return False

        for elem in bs_breadcrumbs.find_all(itemprop="itemListElement"):
            yield {
                "item": elem.find(itemprop="item")["href"],
                "name": elem.find(itemprop="name").string.strip(),
                "position": int(elem.find(itemprop="position")["content"]),
            }
